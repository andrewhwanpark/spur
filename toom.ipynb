{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Read data\n",
    "for filename in glob.iglob(\"sp500_taq/*.dta\"):\n",
    "    # get permnos\n",
    "    df = pd.read_stata(filename)\n",
    "    unique_permnos = df.permno.unique()\n",
    "    print(unique_permnos)\n",
    "    with open(\"permnos.txt\", \"ab\") as f:\n",
    "        np.savetxt(f, unique_permnos, fmt=\"%i\")\n",
    "        \n",
    "# Code has generated permnos.txt file with all the permnos\n",
    "# sort -u permnos.txt >> output.txt\n",
    "# This unix command will only save the unique permnos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e8075e7517a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpermno\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpermnos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_beta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate_beta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PERMNO'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpermno\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mnew_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_beta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Drop irrelevant columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   7083\u001b[0m             \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7084\u001b[0m             \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7085\u001b[0;31m             \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7086\u001b[0m         )\n\u001b[1;32m   7087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mndims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(self, inplace)\u001b[0m\n\u001b[1;32m   5363\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5365\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5366\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5367\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5345\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   5334\u001b[0m         \"\"\"\n\u001b[1;32m   5335\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5336\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5338\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5344\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5345\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5347\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5285\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5286\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5288\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5289\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Leaves year + month, disregards day\n",
    "def new_date(date_old):\n",
    "    return str(date_old)[:6]\n",
    "\n",
    "beta = pd.read_csv('betas.csv')\n",
    "beta.drop(['n', 'RET', 'alpha', 'ivol', 'tvol', 'R2', 'exret'], axis=1, inplace=True)\n",
    "# Keep the last dates of each unique date\n",
    "beta['new_date'] = beta['DATE'].apply(new_date)\n",
    "\n",
    "# New Beta df\n",
    "new_beta = pd.DataFrame()\n",
    "# Unique dates\n",
    "beta_dates = beta.new_date.unique()\n",
    "\n",
    "for date in tqdm(beta_dates):\n",
    "    date_beta = beta.loc[beta['new_date'] == date]\n",
    "    permnos = date_beta.PERMNO.unique()\n",
    "    for permno in permnos:\n",
    "        temp = date_beta.loc[date_beta['PERMNO'] == permno].copy()\n",
    "        new_beta = new_beta.append(temp.tail(1), ignore_index=True)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "new_beta.drop(['new_date'], axis=1, inplace=True)\n",
    "\n",
    "# Function to change dates of beta.csv\n",
    "def convert_date(date_old):\n",
    "    year = int(str(date_old)[:4]) # 2019\n",
    "    month = int(str(date_old)[4:6]) # 02\n",
    "    if month == 12:\n",
    "        # December, so change to next year Jan\n",
    "        date_new = str(year + 1) + '-01'\n",
    "    else:\n",
    "        # Not December\n",
    "        date_new = str(year) + '-' + str(month + 1).zfill(2)\n",
    "    \n",
    "    return date_new\n",
    "\n",
    "# Convert DATE column\n",
    "new_beta['DATE'] = new_beta['DATE'].apply(convert_date)\n",
    "\n",
    "new_beta.to_csv('beta_use.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 300/300 [22:46<00:00,  4.56s/it]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Same as above, but for b_umd\n",
    "\n",
    "# Leaves year + month, disregards day\n",
    "def new_date(date_old):\n",
    "    return str(date_old)[:6]\n",
    "\n",
    "beta = pd.read_csv('umd.csv')\n",
    "beta.drop(['n', 'RET', 'alpha', 'b_mkt', 'b_smb', 'b_hml','ivol', 'tvol', 'R2', 'exret'], axis=1, inplace=True)\n",
    "# Keep the last dates of each unique date\n",
    "beta['new_date'] = beta['DATE'].apply(new_date)\n",
    "\n",
    "# New Beta df\n",
    "new_beta = pd.DataFrame()\n",
    "# Unique dates\n",
    "beta_dates = beta.new_date.unique()\n",
    "\n",
    "for date in tqdm(beta_dates):\n",
    "    date_beta = beta.loc[beta['new_date'] == date]\n",
    "    permnos = date_beta.PERMNO.unique()\n",
    "    for permno in permnos:\n",
    "        temp = date_beta.loc[date_beta['PERMNO'] == permno].copy()\n",
    "        new_beta = new_beta.append(temp.tail(1), ignore_index=True)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "new_beta.drop(['new_date'], axis=1, inplace=True)\n",
    "\n",
    "# Function to change dates of beta.csv\n",
    "def convert_date(date_old):\n",
    "    year = int(str(date_old)[:4]) # 2019\n",
    "    month = int(str(date_old)[4:6]) # 02\n",
    "    if month == 12:\n",
    "        # December, so change to next year Jan\n",
    "        date_new = str(year + 1) + '-01'\n",
    "    else:\n",
    "        # Not December\n",
    "        date_new = str(year) + '-' + str(month + 1).zfill(2)\n",
    "    \n",
    "    return date_new\n",
    "\n",
    "# Convert DATE column\n",
    "new_beta['DATE'] = new_beta['DATE'].apply(convert_date)\n",
    "\n",
    "new_beta.to_csv('umd_use.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 300/300 [1:01:50<00:00, 12.37s/it]\n"
    }
   ],
   "source": [
    "# Same as above, but for ivol and tvol\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Leaves year + month, disregards day\n",
    "def new_date(date_old):\n",
    "    return str(date_old)[:6]\n",
    "\n",
    "beta = pd.read_csv('umd.csv')\n",
    "beta.drop(['n', 'RET', 'alpha', 'b_mkt', 'b_smb', 'b_hml', 'b_umd','R2', 'exret'], axis=1, inplace=True)\n",
    "# Keep the last dates of each unique date\n",
    "beta['new_date'] = beta['DATE'].apply(new_date)\n",
    "\n",
    "# New Beta df\n",
    "new_beta = pd.DataFrame()\n",
    "# Unique dates\n",
    "beta_dates = beta.new_date.unique()\n",
    "\n",
    "for date in tqdm(beta_dates):\n",
    "    date_beta = beta.loc[beta['new_date'] == date]\n",
    "    permnos = date_beta.PERMNO.unique()\n",
    "    for permno in permnos:\n",
    "        temp = date_beta.loc[date_beta['PERMNO'] == permno].copy()\n",
    "        new_beta = new_beta.append(temp.tail(1), ignore_index=True)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "new_beta.drop(['new_date'], axis=1, inplace=True)\n",
    "\n",
    "# Function to change dates of beta.csv\n",
    "def convert_date(date_old):\n",
    "    year = int(str(date_old)[:4]) # 2019\n",
    "    month = int(str(date_old)[4:6]) # 02\n",
    "    if month == 12:\n",
    "        # December, so change to next year Jan\n",
    "        date_new = str(year + 1) + '-01'\n",
    "    else:\n",
    "        # Not December\n",
    "        date_new = str(year) + '-' + str(month + 1).zfill(2)\n",
    "    \n",
    "    return date_new\n",
    "\n",
    "# Convert DATE column\n",
    "new_beta['DATE'] = new_beta['DATE'].apply(convert_date)\n",
    "\n",
    "new_beta.to_csv('vol_use.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign portfolio number\n",
    "new_beta = pd.read_csv('beta_use.csv')\n",
    "mbeta = pd.DataFrame()\n",
    "strategies = ['b_mkt', 'b_smb', 'b_hml']\n",
    "\n",
    "# Assign portfolio numbers only to permnos that belong in specific date\n",
    "for filename in glob.iglob(\"sp500_taq/*.dta\"):\n",
    "    df = pd.read_stata(filename)\n",
    "    # Get unique dates for this file\n",
    "    unique_dates = df.td.unique()\n",
    "    # Isolate permnos for dates\n",
    "    for date in unique_dates:\n",
    "        date_df = df.loc[df['td'] == date]\n",
    "        date_permnos = date_df.permno.unique()\n",
    "        # Bucket betas\n",
    "        date_beta = new_beta.loc[new_beta['DATE'].str.match(str(date)[:7]) & new_beta['PERMNO'].isin(date_permnos)].copy()\n",
    "        # Iterate all strategies\n",
    "        date_beta['b_mkt_port'] = pd.qcut(date_beta['b_mkt'], 5, labels=False, duplicates='drop')\n",
    "        date_beta['b_smb_port'] = pd.qcut(date_beta['b_smb'], 5, labels=False, duplicates='drop')\n",
    "        date_beta['b_hml_port'] = pd.qcut(date_beta['b_hml'], 5, labels=False, duplicates='drop')\n",
    "\n",
    "        mbeta = mbeta.append(date_beta, ignore_index=True)\n",
    "\n",
    "mbeta.to_csv('isolated_beta_port.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but for b_umd\n",
    "\n",
    "# Assign portfolio number\n",
    "new_beta = pd.read_csv('umd_use.csv')\n",
    "mbeta = pd.DataFrame()\n",
    "\n",
    "# Assign portfolio numbers only to permnos that belong in specific date\n",
    "for filename in glob.iglob(\"sp500_taq/*.dta\"):\n",
    "    df = pd.read_stata(filename)\n",
    "    # Get unique dates for this file\n",
    "    unique_dates = df.td.unique()\n",
    "    # Isolate permnos for dates\n",
    "    for date in unique_dates:\n",
    "        date_df = df.loc[df['td'] == date]\n",
    "        date_permnos = date_df.permno.unique()\n",
    "        # Bucket betas\n",
    "        date_beta = new_beta.loc[new_beta['DATE'].str.match(str(date)[:7]) & new_beta['PERMNO'].isin(date_permnos)].copy()\n",
    "        # Iterate all strategies\n",
    "        date_beta['b_umd_port'] = pd.qcut(date_beta['b_umd'], 5, labels=False, duplicates='drop')\n",
    "\n",
    "        mbeta = mbeta.append(date_beta, ignore_index=True)\n",
    "\n",
    "mbeta.to_csv('isolated_umd_port.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Same as above but for ivol and tvol\n",
    "\n",
    "# Assign portfolio number\n",
    "new_beta = pd.read_csv('vol_use.csv')\n",
    "\n",
    "mbeta = pd.DataFrame()\n",
    "\n",
    "# Assign portfolio numbers only to permnos that belong in specific date\n",
    "for filename in glob.iglob(\"sp500_taq/*.dta\"):\n",
    "    df = pd.read_stata(filename)\n",
    "    # Get unique dates for this file\n",
    "    unique_dates = df.td.unique()\n",
    "    # Isolate permnos for dates\n",
    "    for date in unique_dates:\n",
    "        date_df = df.loc[df['td'] == date]\n",
    "        date_permnos = date_df.permno.unique()\n",
    "        # Bucket betas\n",
    "        date_beta = new_beta.loc[new_beta['DATE'].str.match(str(date)[:7]) & new_beta['PERMNO'].isin(date_permnos)].copy()\n",
    "        # Iterate all strategies\n",
    "        date_beta['ivol_port'] = pd.qcut(date_beta['ivol'], 5, labels=False, duplicates='drop')\n",
    "        date_beta['tvol_port'] = pd.qcut(date_beta['tvol'], 5, labels=False, duplicates='drop')\n",
    "\n",
    "        mbeta = mbeta.append(date_beta, ignore_index=True)\n",
    "\n",
    "mbeta.to_csv('isolated_vol_port.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "\n\n  0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[Aloop\n\n\n  0%|          | 1/200 [00:15<51:09, 15.42s/it]\u001b[A\u001b[Aloop\n\n\n  1%|          | 2/200 [00:18<39:07, 11.85s/it]\u001b[A\u001b[Aloop\n\n\n  2%|▏         | 3/200 [00:22<30:45,  9.37s/it]\u001b[A\u001b[Aloop\n\n\n  2%|▏         | 4/200 [00:26<24:55,  7.63s/it]\u001b[A\u001b[Aloop\n\n\n  2%|▎         | 5/200 [00:29<20:49,  6.41s/it]\u001b[A\u001b[Aloop\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-745ce8c81365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# df is the price data for this announcement day\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'td'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# price data for corresponding day\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mpdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_date\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# clean col tr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1950\u001b[0m                 )\n\u001b[1;32m   1951\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(self, pat, case, flags, na)\u001b[0m\n\u001b[1;32m   2766\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mforbid_nonstring_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bytes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2768\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2769\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36mstr_match\u001b[0;34m(arr, pat, case, flags, na)\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_na_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m_na_map\u001b[0;34m(f, arr, na_result, dtype)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mna_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_map_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m_map_object\u001b[0;34m(f, arr, na_mask, na_value, dtype)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# Reraise the exception if callable `f` got wrong number of args.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer_mask\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_na_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Final run\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Function to convert date into concise string format\n",
    "# Ex: 2019-01-01:00:00:00 into 2019-01-01\n",
    "def convert_date(date_oldformat):\n",
    "    date_oldformat = str(date_oldformat)\n",
    "    return date_oldformat[11:]\n",
    "\n",
    "# Calculate hour/min of 1 minute before announcement\n",
    "def get_minbefore(atime):\n",
    "    ahour = int(str(atime)[:2])\n",
    "    amin = int(str(atime)[3:5])\n",
    "    atime = datetime.datetime(2000,1,1,ahour,amin) - datetime.timedelta(minutes=1)\n",
    "    atime = str(atime)[11:] # atime is now the 1 minute before announcement time\n",
    "    return atime\n",
    "\n",
    "# Master dataframe with columns\n",
    "masterdf = pd.DataFrame(columns=['date', 'b_mkt_pre_ls', 'b_mkt_post_ls', 'b_smb_pre_ls', 'b_smb_post_ls', 'b_hml_pre_ls', 'b_hml_post_ls'])\n",
    "\n",
    "# Handle FOMC file\n",
    "fomc = pd.read_excel('fomc_meetings.xlsx') # open fomc file\n",
    "fomc = fomc[fomc['announcement'].notna()] # drop rows with no announcement time\n",
    "fomc = fomc.loc[fomc['fomc'] == 1] # drop rows with fomc col set 0\n",
    "\n",
    "fomc_dates = fomc.td.unique() # unique annoucement dates\n",
    "\n",
    "# Open beta portfolios\n",
    "beta = pd.read_csv('isolated_beta_port.csv')\n",
    "\n",
    "strategies = ['b_mkt','b_smb','b_hml'] # strategies to iterate\n",
    "\n",
    "# Store previous year of fomc announcement years to prevent unnecessarily opening data\n",
    "prev_year = 0\n",
    "\n",
    "for date in tqdm(fomc_dates):\n",
    "    year = str(date)[:4] # ex: 2019\n",
    "    year_month = str(date)[:7] # ex: 2019-01\n",
    "    time = str(date)[:10] # ex: 2019-01-01\n",
    "    \n",
    "    # If file already open, skip\n",
    "    if year != prev_year:\n",
    "        filename = 'sp500_taq/sp500_fomc_' + year + '.dta' # create filename\n",
    "        df = pd.read_stata(filename) # Open price data for the year / strat\n",
    "    \n",
    "    # Store previous year\n",
    "    prev_year = year\n",
    "\n",
    "    df['td'] = df['td'].astype('str') # convert col into string from datetime\n",
    "    \n",
    "    # df is the price data for this announcement day\n",
    "    pdf = df.loc[df['td'].str.match(time)].copy() # price data for corresponding day\n",
    "    pdf['tr'] = pdf['tr'].apply(convert_date) # clean col tr\n",
    "\n",
    "    # Get announcement time\n",
    "    atime = fomc.loc[fomc['td'] == date, 'announcement'].item()\n",
    "    # Get one minute before announcement time\n",
    "    atime = get_minbefore(atime)\n",
    "\n",
    "    # Get beta for date\n",
    "    date_beta = beta.loc[beta['DATE'] == year_month]\n",
    "\n",
    "    temp = []\n",
    "\n",
    "    for strat in strategies:\n",
    "        col_name = strat + '_port'\n",
    "        # Permnos of portfolio n\n",
    "        long_permnos = date_beta.loc[date_beta[col_name] == 4, 'PERMNO'].values\n",
    "        short_permnos = date_beta.loc[date_beta[col_name] == 0, 'PERMNO'].values\n",
    "        # Isolate portfolio 0 and 4 (bottom and top quintile)\n",
    "        long_df = pdf[pdf['permno'].isin(long_permnos)].copy()\n",
    "        short_df = pdf[pdf['permno'].isin(short_permnos)].copy()\n",
    "    \n",
    "        # List of unique permnos for each portfolio\n",
    "        long_permnos = long_df.permno.unique()\n",
    "        short_permnos = short_df.permno.unique()\n",
    "\n",
    "        # Store returns for each date here\n",
    "        l_pre = [] # open to 1 min before announcement\n",
    "        l_post = [] # 1 min before announcment to close\n",
    "\n",
    "        s_pre = []\n",
    "        s_post = []\n",
    "    \n",
    "        # Long 4\n",
    "        for permno in long_permnos:\n",
    "            permno_df = long_df.loc[long_df['permno'] == permno]\n",
    "            \n",
    "            # Open iprice\n",
    "            open_iprice = permno_df.loc[permno_df['tr'] == '09:30:00', 'iprice'].values[0]\n",
    "            # 1 min before announcement iprice\n",
    "            pre_iprice = permno_df.loc[permno_df['tr'] == atime, 'iprice'].values[0]\n",
    "            # End iprice\n",
    "            close_iprice = permno_df.loc[permno_df['tr'] == '16:00:00', 'iprice'].values[0]\n",
    "            \n",
    "            # Open to pre-announcement HPR\n",
    "            open_to_pre = (pre_iprice / open_iprice) - 1\n",
    "            pre_to_close = (close_iprice / pre_iprice) - 1\n",
    "        \n",
    "            l_pre.append(open_to_pre)\n",
    "            l_post.append(pre_to_close)\n",
    "\n",
    "        # Short 0\n",
    "        for permno in short_permnos:\n",
    "            permno_df = short_df.loc[short_df['permno'] == permno]\n",
    "            \n",
    "            # Open iprice\n",
    "            open_iprice = permno_df.loc[permno_df['tr'] == '09:30:00', 'iprice'].values[0]\n",
    "            # 1 min before announcement iprice\n",
    "            pre_iprice = permno_df.loc[permno_df['tr'] == atime, 'iprice'].values[0]\n",
    "            # End iprice\n",
    "            close_iprice = permno_df.loc[permno_df['tr'] == '16:00:00', 'iprice'].values[0]\n",
    "            \n",
    "            # Open to pre-announcement HPR\n",
    "            open_to_pre = (pre_iprice / open_iprice) - 1\n",
    "            pre_to_close = (close_iprice / pre_iprice) - 1\n",
    "        \n",
    "            s_pre.append(open_to_pre)\n",
    "            s_post.append(pre_to_close)\n",
    "        \n",
    "        # Now, the returns for the announcement date are calculated\n",
    "        l_pre_avg = sum(l_pre) / len(l_pre)\n",
    "        l_post_avg = sum(l_post) / len(l_post)\n",
    "\n",
    "        s_pre_avg = sum(s_pre) / len(s_pre)\n",
    "        s_post_avg = sum(s_post) / len(s_post)\n",
    "        \n",
    "        temp.append(l_pre_avg - s_pre_avg)\n",
    "        temp.append(l_post_avg - s_post_avg)\n",
    "\n",
    "    # Append row\n",
    "    masterdf = masterdf.append({'date': date, 'b_mkt_pre_ls': temp[0], 'b_mkt_post_ls': temp[1], 'b_smb_pre_ls': temp[2], 'b_smb_post_ls': temp[3], 'b_hml_pre_ls': temp[4], 'b_hml_post_ls': temp[5]}, ignore_index=True)\n",
    "\n",
    "# Export as csv\n",
    "masterdf.to_csv('announcement_ret.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 200/200 [11:54<00:00,  3.57s/it]\n"
    }
   ],
   "source": [
    "# LS Portfolio Return Calculation for b_umd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Function to convert date into concise string format\n",
    "# Ex: 2019-01-01:00:00:00 into 2019-01-01\n",
    "def convert_date(date_oldformat):\n",
    "    date_oldformat = str(date_oldformat)\n",
    "    return date_oldformat[11:]\n",
    "\n",
    "# Calculate hour/min of 1 minute before announcement\n",
    "def get_minbefore(atime):\n",
    "    ahour = int(str(atime)[:2])\n",
    "    amin = int(str(atime)[3:5])\n",
    "    atime = datetime.datetime(2000,1,1,ahour,amin) - datetime.timedelta(minutes=1)\n",
    "    atime = str(atime)[11:] # atime is now the 1 minute before announcement time\n",
    "    return atime\n",
    "\n",
    "# Master dataframe with columns\n",
    "masterdf = pd.DataFrame(columns=['date', 'b_umd_pre_ls', 'b_umd_post_ls'])\n",
    "\n",
    "# Handle FOMC file\n",
    "fomc = pd.read_excel('fomc_meetings.xlsx') # open fomc file\n",
    "fomc = fomc[fomc['announcement'].notna()] # drop rows with no announcement time\n",
    "fomc = fomc.loc[fomc['fomc'] == 1] # drop rows with fomc col set 0\n",
    "\n",
    "fomc_dates = fomc.td.unique() # unique annoucement dates\n",
    "\n",
    "# Open beta portfolios\n",
    "beta = pd.read_csv('isolated_umd_port.csv')\n",
    "\n",
    "strategies = ['b_umd'] # strategies to iterate\n",
    "\n",
    "# Store previous year of fomc announcement years to prevent unnecessarily opening data\n",
    "prev_year = 0\n",
    "\n",
    "for date in tqdm(fomc_dates):\n",
    "    year = str(date)[:4] # ex: 2019\n",
    "    year_month = str(date)[:7] # ex: 2019-01\n",
    "    time = str(date)[:10] # ex: 2019-01-01\n",
    "    \n",
    "    # If file already open, skip\n",
    "    if year != prev_year:\n",
    "        filename = 'sp500_taq/sp500_fomc_' + year + '.dta' # create filename\n",
    "        df = pd.read_stata(filename) # Open price data for the year / strat\n",
    "    \n",
    "    # Store previous year\n",
    "    prev_year = year\n",
    "\n",
    "    df['td'] = df['td'].astype('str') # convert col into string from datetime\n",
    "    \n",
    "    # df is the price data for this announcement day\n",
    "    pdf = df.loc[df['td'].str.match(time)].copy() # price data for corresponding day\n",
    "    pdf['tr'] = pdf['tr'].apply(convert_date) # clean col tr\n",
    "\n",
    "    # Get announcement time\n",
    "    atime = fomc.loc[fomc['td'] == date, 'announcement'].item()\n",
    "    # Get one minute before announcement time\n",
    "    atime = get_minbefore(atime)\n",
    "\n",
    "    # Get beta for date\n",
    "    date_beta = beta.loc[beta['DATE'] == year_month]\n",
    "\n",
    "    temp = []\n",
    "\n",
    "    for strat in strategies:\n",
    "        col_name = strat + '_port'\n",
    "        # Permnos of portfolio n\n",
    "        long_permnos = date_beta.loc[date_beta[col_name] == 4, 'PERMNO'].values\n",
    "        short_permnos = date_beta.loc[date_beta[col_name] == 0, 'PERMNO'].values\n",
    "        # Isolate portfolio 0 and 4 (bottom and top quintile)\n",
    "        long_df = pdf[pdf['permno'].isin(long_permnos)].copy()\n",
    "        short_df = pdf[pdf['permno'].isin(short_permnos)].copy()\n",
    "    \n",
    "        # List of unique permnos for each portfolio\n",
    "        long_permnos = long_df.permno.unique()\n",
    "        short_permnos = short_df.permno.unique()\n",
    "\n",
    "        # Store returns for each date here\n",
    "        l_pre = [] # open to 1 min before announcement\n",
    "        l_post = [] # 1 min before announcment to close\n",
    "\n",
    "        s_pre = []\n",
    "        s_post = []\n",
    "    \n",
    "        # Long 4\n",
    "        for permno in long_permnos:\n",
    "            permno_df = long_df.loc[long_df['permno'] == permno]\n",
    "            \n",
    "            # Open iprice\n",
    "            open_iprice = permno_df.loc[permno_df['tr'] == '09:30:00', 'iprice'].values[0]\n",
    "            # 1 min before announcement iprice\n",
    "            pre_iprice = permno_df.loc[permno_df['tr'] == atime, 'iprice'].values[0]\n",
    "            # End iprice\n",
    "            close_iprice = permno_df.loc[permno_df['tr'] == '16:00:00', 'iprice'].values[0]\n",
    "            \n",
    "            # Open to pre-announcement HPR\n",
    "            open_to_pre = (pre_iprice / open_iprice) - 1\n",
    "            pre_to_close = (close_iprice / pre_iprice) - 1\n",
    "        \n",
    "            l_pre.append(open_to_pre)\n",
    "            l_post.append(pre_to_close)\n",
    "\n",
    "        # Short 0\n",
    "        for permno in short_permnos:\n",
    "            permno_df = short_df.loc[short_df['permno'] == permno]\n",
    "            \n",
    "            # Open iprice\n",
    "            open_iprice = permno_df.loc[permno_df['tr'] == '09:30:00', 'iprice'].values[0]\n",
    "            # 1 min before announcement iprice\n",
    "            pre_iprice = permno_df.loc[permno_df['tr'] == atime, 'iprice'].values[0]\n",
    "            # End iprice\n",
    "            close_iprice = permno_df.loc[permno_df['tr'] == '16:00:00', 'iprice'].values[0]\n",
    "            \n",
    "            # Open to pre-announcement HPR\n",
    "            open_to_pre = (pre_iprice / open_iprice) - 1\n",
    "            pre_to_close = (close_iprice / pre_iprice) - 1\n",
    "        \n",
    "            s_pre.append(open_to_pre)\n",
    "            s_post.append(pre_to_close)\n",
    "        \n",
    "        # Now, the returns for the announcement date are calculated\n",
    "        l_pre_avg = sum(l_pre) / len(l_pre)\n",
    "        l_post_avg = sum(l_post) / len(l_post)\n",
    "\n",
    "        s_pre_avg = sum(s_pre) / len(s_pre)\n",
    "        s_post_avg = sum(s_post) / len(s_post)\n",
    "        \n",
    "        temp.append(l_pre_avg - s_pre_avg)\n",
    "        temp.append(l_post_avg - s_post_avg)\n",
    "\n",
    "    # Append row\n",
    "    masterdf = masterdf.append({'date': date, 'b_umd_pre_ls': temp[0], 'b_umd_post_ls': temp[1]}, ignore_index=True)\n",
    "\n",
    "# Export as csv\n",
    "masterdf.to_csv('umd_ret.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 200/200 [13:40<00:00,  4.10s/it]\n"
    }
   ],
   "source": [
    "# Calculate LS portfolio returns for ivol & tvol\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Function to convert date into concise string format\n",
    "# Ex: 2019-01-01:00:00:00 into 2019-01-01\n",
    "def convert_date(date_oldformat):\n",
    "    date_oldformat = str(date_oldformat)\n",
    "    return date_oldformat[11:]\n",
    "\n",
    "# Calculate hour/min of 1 minute before announcement\n",
    "def get_minbefore(atime):\n",
    "    ahour = int(str(atime)[:2])\n",
    "    amin = int(str(atime)[3:5])\n",
    "    atime = datetime.datetime(2000,1,1,ahour,amin) - datetime.timedelta(minutes=1)\n",
    "    atime = str(atime)[11:] # atime is now the 1 minute before announcement time\n",
    "    return atime\n",
    "\n",
    "# Master dataframe with columns\n",
    "masterdf = pd.DataFrame(columns=['date', 'ivol_pre_ls', 'ivol_post_ls', 'tvol_pre_ls', 'tvol_post_ls'])\n",
    "\n",
    "# Handle FOMC file\n",
    "fomc = pd.read_excel('fomc_meetings.xlsx') # open fomc file\n",
    "fomc = fomc[fomc['announcement'].notna()] # drop rows with no announcement time\n",
    "fomc = fomc.loc[fomc['fomc'] == 1] # drop rows with fomc col set 0\n",
    "\n",
    "fomc_dates = fomc.td.unique() # unique annoucement dates\n",
    "\n",
    "# Open beta portfolios\n",
    "beta = pd.read_csv('isolated_vol_port.csv')\n",
    "\n",
    "strategies = ['ivol', 'tvol'] # strategies to iterate\n",
    "\n",
    "# Store previous year of fomc announcement years to prevent unnecessarily opening data\n",
    "prev_year = 0\n",
    "\n",
    "for date in tqdm(fomc_dates):\n",
    "    year = str(date)[:4] # ex: 2019\n",
    "    year_month = str(date)[:7] # ex: 2019-01\n",
    "    time = str(date)[:10] # ex: 2019-01-01\n",
    "    \n",
    "    # If file already open, skip\n",
    "    if year != prev_year:\n",
    "        filename = 'sp500_taq/sp500_fomc_' + year + '.dta' # create filename\n",
    "        df = pd.read_stata(filename) # Open price data for the year / strat\n",
    "    \n",
    "    # Store previous year\n",
    "    prev_year = year\n",
    "\n",
    "    df['td'] = df['td'].astype('str') # convert col into string from datetime\n",
    "    \n",
    "    # df is the price data for this announcement day\n",
    "    pdf = df.loc[df['td'].str.match(time)].copy() # price data for corresponding day\n",
    "    pdf['tr'] = pdf['tr'].apply(convert_date) # clean col tr\n",
    "\n",
    "    # Get announcement time\n",
    "    atime = fomc.loc[fomc['td'] == date, 'announcement'].item()\n",
    "    # Get one minute before announcement time\n",
    "    atime = get_minbefore(atime)\n",
    "\n",
    "    # Get beta for date\n",
    "    date_beta = beta.loc[beta['DATE'] == year_month]\n",
    "\n",
    "    temp = []\n",
    "\n",
    "    for strat in strategies:\n",
    "        col_name = strat + '_port'\n",
    "        # Permnos of portfolio n\n",
    "        long_permnos = date_beta.loc[date_beta[col_name] == 4, 'PERMNO'].values\n",
    "        short_permnos = date_beta.loc[date_beta[col_name] == 0, 'PERMNO'].values\n",
    "        # Isolate portfolio 0 and 4 (bottom and top quintile)\n",
    "        long_df = pdf[pdf['permno'].isin(long_permnos)].copy()\n",
    "        short_df = pdf[pdf['permno'].isin(short_permnos)].copy()\n",
    "    \n",
    "        # List of unique permnos for each portfolio\n",
    "        long_permnos = long_df.permno.unique()\n",
    "        short_permnos = short_df.permno.unique()\n",
    "\n",
    "        # Store returns for each date here\n",
    "        l_pre = [] # open to 1 min before announcement\n",
    "        l_post = [] # 1 min before announcment to close\n",
    "\n",
    "        s_pre = []\n",
    "        s_post = []\n",
    "    \n",
    "        # Long 4\n",
    "        for permno in long_permnos:\n",
    "            permno_df = long_df.loc[long_df['permno'] == permno]\n",
    "            \n",
    "            # Open iprice\n",
    "            open_iprice = permno_df.loc[permno_df['tr'] == '09:30:00', 'iprice'].values[0]\n",
    "            # 1 min before announcement iprice\n",
    "            pre_iprice = permno_df.loc[permno_df['tr'] == atime, 'iprice'].values[0]\n",
    "            # End iprice\n",
    "            close_iprice = permno_df.loc[permno_df['tr'] == '16:00:00', 'iprice'].values[0]\n",
    "            \n",
    "            # Open to pre-announcement HPR\n",
    "            open_to_pre = (pre_iprice / open_iprice) - 1\n",
    "            pre_to_close = (close_iprice / pre_iprice) - 1\n",
    "        \n",
    "            l_pre.append(open_to_pre)\n",
    "            l_post.append(pre_to_close)\n",
    "\n",
    "        # Short 0\n",
    "        for permno in short_permnos:\n",
    "            permno_df = short_df.loc[short_df['permno'] == permno]\n",
    "            \n",
    "            # Open iprice\n",
    "            open_iprice = permno_df.loc[permno_df['tr'] == '09:30:00', 'iprice'].values[0]\n",
    "            # 1 min before announcement iprice\n",
    "            pre_iprice = permno_df.loc[permno_df['tr'] == atime, 'iprice'].values[0]\n",
    "            # End iprice\n",
    "            close_iprice = permno_df.loc[permno_df['tr'] == '16:00:00', 'iprice'].values[0]\n",
    "            \n",
    "            # Open to pre-announcement HPR\n",
    "            open_to_pre = (pre_iprice / open_iprice) - 1\n",
    "            pre_to_close = (close_iprice / pre_iprice) - 1\n",
    "        \n",
    "            s_pre.append(open_to_pre)\n",
    "            s_post.append(pre_to_close)\n",
    "        \n",
    "        # Now, the returns for the announcement date are calculated\n",
    "        l_pre_avg = sum(l_pre) / len(l_pre)\n",
    "        l_post_avg = sum(l_post) / len(l_post)\n",
    "\n",
    "        s_pre_avg = sum(s_pre) / len(s_pre)\n",
    "        s_post_avg = sum(s_post) / len(s_post)\n",
    "        \n",
    "        temp.append(l_pre_avg - s_pre_avg)\n",
    "        temp.append(l_post_avg - s_post_avg)\n",
    "\n",
    "    # Append row\n",
    "    masterdf = masterdf.append({'date': date, 'ivol_pre_ls': temp[0], 'ivol_post_ls': temp[1], 'tvol_pre_ls': temp[2], 'tvol_post_ls': temp[3]}, ignore_index=True)\n",
    "\n",
    "# Export as csv\n",
    "masterdf.to_csv('vol_ret.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Clean output by rounding to 8 significant digits after converting to percentage from raw returns\n",
    "def clean(old_ret):\n",
    "    new_ret = round(old_ret * 100, 8)\n",
    "    return new_ret\n",
    "\n",
    "df = pd.read_csv('announcement_ret.csv')\n",
    "\n",
    "df[['b_mkt_pre_ls', 'b_mkt_post_ls', 'b_smb_pre_ls', 'b_smb_post_ls', 'b_hml_pre_ls', 'b_hml_post_ls']] = df[['b_mkt_pre_ls', 'b_mkt_post_ls', 'b_smb_pre_ls', 'b_smb_post_ls', 'b_hml_pre_ls', 'b_hml_post_ls']].apply(clean)\n",
    "df.to_csv('announcement_ret.csv', encoding='utf-8', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean output for b_umd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Clean output by rounding to 8 significant digits after converting to percentage from raw returns\n",
    "def clean(old_ret):\n",
    "    new_ret = round(old_ret * 100, 8)\n",
    "    return new_ret\n",
    "\n",
    "df = pd.read_csv('umd_ret.csv')\n",
    "\n",
    "df[['b_umd_pre_ls', 'b_umd_post_ls']] = df[['b_umd_pre_ls', 'b_umd_post_ls']].apply(clean)\n",
    "df.to_csv('umd_ret.csv', encoding='utf-8', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean output for ivol, tvol\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Clean output by rounding to 8 significant digits after converting to percentage from raw returns\n",
    "def clean(old_ret):\n",
    "    new_ret = round(old_ret * 100, 8)\n",
    "    return new_ret\n",
    "\n",
    "df = pd.read_csv('vol_ret.csv')\n",
    "\n",
    "df[['ivol_pre_ls', 'ivol_post_ls', 'tvol_pre_ls', 'tvol_post_ls']] = df[['ivol_pre_ls', 'ivol_post_ls', 'tvol_pre_ls', 'tvol_post_ls']].apply(clean)\n",
    "df.to_csv('vol_ret.csv', encoding='utf-8', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitf3c259cf72ce47359da3fbfde37f9527"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}